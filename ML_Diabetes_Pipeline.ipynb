{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": { "provenance": [] },
    "kernelspec": { "name": "python3", "display_name": "Python 3" },
    "language_info": { "name": "python" }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/siddmuns/diabetes-ml-pipeline.git\n",
        "%cd diabetes-ml-pipeline\n",
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure artifacts folders exist\n",
        "import os\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "os.makedirs(\"mlruns\", exist_ok=True)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and split data\n",
        "from data_ingestion import preprocess_and_split\n",
        "X_train, X_valid, X_test, y_train, y_valid, y_test, scaler, feature_names = preprocess_and_split(\n",
        "    \"diabetes.csv\", out_dir=\"artifacts\"\n",
        ")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup MLflow\n",
        "from tune_and_train import setup_mlflow\n",
        "mlflow = setup_mlflow(local_dir=\"mlruns\", experiment_name=\"Diabetes_Pipeline\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Optuna hyperparameter tuning with fixed trial logging\n",
        "from tune_and_train import run_optuna\n",
        "study = run_optuna(X_train, y_train, X_valid, y_valid, n_trials=20)\n",
        "best_params = study.best_trial.params\n",
        "print(\"Best Optuna parameters:\", best_params)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain final model on train+valid and log correctly\n",
        "from tune_and_train import retrain_final_and_log\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "\n",
        "X_train_full = np.vstack([X_train, X_valid])\n",
        "y_train_full = np.concatenate([y_train, y_valid])\n",
        "\n",
        "with mlflow.start_run(run_name=\"Final_Model\") as run:\n",
        "    model, test_acc, test_auc, train_time, model_path = retrain_final_and_log(\n",
        "        X_train_full, y_train_full, X_test, y_test, best_params, artifacts_dir=\"artifacts\"\n",
        "    )\n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metrics({\n",
        "        \"test_accuracy\": float(test_acc),\n",
        "        \"test_roc_auc\": float(test_auc),\n",
        "        \"train_time_s\": float(train_time)\n",
        "    })\n",
        "    \n",
        "    example_input = pd.DataFrame(X_train_full[:5], columns=[f\"feature_{i}\" for i in range(X_train_full.shape[1])])\n",
        "    from mlflow.models.signature import infer_signature\n",
        "    signature = infer_signature(X_train_full, model.predict(X_train_full))\n",
        "    mlflow.sklearn.log_model(model, name=\"final_model\", input_example=example_input, signature=signature)\n",
        "\n",
        "    run_id = run.info.run_id\n",
        "    print(\"Final model logged, run_id:\", run_id)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: register model in MLflow Model Registry\n",
        "from mlflow import MlflowClient\n",
        "from tune_and_train import register_model_mlflow\n",
        "client = MlflowClient()\n",
        "register_model_mlflow(run_id, client, model_name=\"Diabetes_GB_Model\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally as backup\n",
        "import joblib\n",
        "joblib.dump(model, \"artifacts/gb_final_model.pkl\")\n",
        "joblib.dump(scaler, \"artifacts/scaler.pkl\")\n",
        "print(\"Saved gb_final_model.pkl and scaler.pkl\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch predictions\n",
        "from batch_inference import batch_predict\n",
        "out = batch_predict(\"artifacts/gb_final_model.pkl\", \"artifacts/scaler.pkl\", \"diabetes2.csv\", \"artifacts/predictions.csv\")\n",
        "out.head()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drift detection\n",
        "from drift_detection import detect_and_log_drift\n",
        "rpt = detect_and_log_drift(\"diabetes.csv\", \"diabetes2.csv\", out_dir=\"artifacts\")\n",
        "print(rpt)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize and evaluate\n",
        "from visualize_and_evaluate import make_plots_and_log\n",
        "make_plots_and_log(model, X_test, y_test, feature_names, run_name=\"Final_Plots\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}

